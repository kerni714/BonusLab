---
title: "BostonHousing"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{BostonHousing}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
Attach libraries:
```{r setup}
library(BonusLab)
library(mlbench)
library(caret)
```

Load and preview the dataset:
```{r}
data(BostonHousing)
dim(BostonHousing)
head(BostonHousing)
```
1. Divide the dataset into training and test set:
```{r}
set.seed(42)
inTrain <- caret::createDataPartition(y = BostonHousing$medv, p = .75,list = FALSE)

training <- BostonHousing[ inTrain,]
testing <- BostonHousing[-inTrain,]
nrow(training)
nrow(testing)
```
Set up sampling methods, none and 10-fold cross-validation:
```{r}
set.seed(42)
sampling_none <- trainControl(method = "none")
sampling_cv_10 <- trainControl(method = "cv",p=90)

```
2.Fit linear models, all variables and forward selection of covariates\
All variables:
```{r}
#- Training set
lm_res_training <- train(medv ~ .,data = training, method = 'lm', trControl = sampling_none)
lm_res_training

#summary(lm_res_training)

#- 10 fold cross-validation
lm_res_cv_10 <- train(medv ~ .,data = training, method = 'lm', trControl = sampling_cv_10)
lm_res_cv_10
```
Forward selection of covariates:
```{r}
# Set up paramter for number of variables in subsets
grid <- data.frame(c(1:(length(BostonHousing)-1)))
names(grid) <- c("nvmax")

lmsubset_res_cv_10 <- train(medv ~ .,data = training,intercept=TRUE,
                            method = 'leapForward',tuneGrid=grid, trControl = sampling_cv_10)
lmsubset_res_cv_10

summary(lmsubset_res_cv_10)
```
Fit model with selected variables on whole training set:
```{r}
lmsubset_res_training <- train(medv ~ crim+zn+chas+nox+rm+dis+rad+tax+ptratio+b+lstat,data = training,
                               method = 'lm',trControl = sampling_none)
lmsubset_res_training

#summary(lmsubset_res_training)

```

3) Evaluate the performance of these models on the training set:
```{r}
#- Compare RMSE from all of training set
lm_training_error <- training$medv-predict(lm_res_training)
lmsubset_training_error <- training$medv-predict(lmsubset_res_training)
rmse_training_summary_lm <- c(lm=rmse(lm_training_error),lmsubset=rmse(lmsubset_training_error))
print(rmse_training_summary_lm)

#- Compare RMSE from cross-validation rounds
lm_vs_lmsubset_cv_10 <- resamples(list(lm = lm_res_cv_10, lm_subset = lmsubset_res_cv_10))
summary(lm_vs_lmsubset_cv_10)
```
4) Fit a ridge regression model using your ridgereg() function to the training dataset for different values of λ.

Set parameters for running ridgereg() with caret:
```{r}
#- Start information:
ridgereg <- list(type = "Regression",
                 library = "BonusLab",
                 loop = NULL)
#- Parameters:
prm <- data.frame(parameter = c("lambda"),
                  class = c("numeric"),
                  label = c("lambda"))
ridgereg$parameters <- prm
#- Grid
rrGrid <- function(x, y, len = NULL, search = "grid") {
  if(search == "grid") {
    l1 <- c(0, 0.1, 0.5)
    l2 <- 1:20
    l <- c(l1,l2)
    out <- data.frame(lambda = l)
  }
}
ridgereg$grid <- rrGrid
#- Fit
rrFit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  formula <- y ~ .
  data <- as.data.frame(cbind(x,y))
  lambda <- param$lambda
  BonusLab::ridgereg(formula, data, lambda)
}
ridgereg$fit <- rrFit
#- Pred
rrPred <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {
  newdata <- as.data.frame(newdata)
  predict(modelFit, newdata)
}
ridgereg$predict <- rrPred
#- Prob
rrProb <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {
  NULL
}
ridgereg$prob <- rrProb
```
Train model for some different values of λ:

```{r}
#lambda=1
grid <-data.frame(1)
names(grid) <- c("lambda")

rr_res_lambda_1 <- train(medv ~ ., data = training,
                         method = ridgereg,
                         preProc = c("center", "scale"),
                         tuneGrid=grid,
                         trControl = sampling_none)
rr_res_lambda_1

#lambda=5
grid <-data.frame(5)
names(grid) <- c("lambda")

rr_res_lambda_5 <- train(medv ~ ., data = training,
                         method = ridgereg,
                         preProc = c("center", "scale"),
                         tuneGrid=grid,
                         trControl = sampling_none)
rr_res_lambda_5

rr_res_training_lambda_1 <- training$medv-predict(rr_res_lambda_1)
rr_res_training_lambda_5 <- training$medv-predict(rr_res_lambda_5)

rmse_summary_rr_training <- c(ridge_lambda_1=rmse(rr_res_training_lambda_1),
    ridge_lambda_5=rmse(rr_res_training_lambda_5))

print(rmse_summary_rr_training)

```
5. Find the best hyperparameter value for λ using 10-fold cross-validation on the training set.
```{r}
rr_res_cv_10 <- train(medv ~ ., data = training,
                      method = ridgereg,
                      preProc = c("center", "scale"),
                      trControl = sampling_cv_10)
rr_res_cv_10
```

6. Evaluate the performance of all three models on the test dataset:

```{r}
lm_test_error <- testing$medv-predict(lm_res_training, newdata = testing)
lmsubset_test_error <- testing$medv-predict(lmsubset_res_cv_10, newdata = testing)
rr_test_error <- testing$medv-predict(rr_res_cv_10, newdata = testing)

rmse_summary <- c(lm=rmse(lm_test_error),lmsubset=rmse(lmsubset_test_error),ridge=rmse(rr_test_error))

print(rmse_summary)
```
Ridge regression has the lowest RMSE for the test set, followed by the linear model subset and finally the linear model with all covariates.
